{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28bISA4b5D_I"
      },
      "source": [
        "# Resnet\n",
        "\n",
        "### Introduction\n",
        "\n",
        "**ResNet is a network structure proposed by the He Kaiming, Sun Jian and others of Microsoft Research Asia in 2015, and won the first place in the ILSVRC-2015 classification task. At the same time, it won the first place in ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation tasks. It was a sensation at the time.**\n",
        "\n",
        "ResNet, also known as residual neural network, refers to the idea of ​​adding residual learning to the traditional convolutional neural network, which solves the problem of gradient dispersion and accuracy degradation (training set) in deep networks, so that the network can get more and more The deeper, both the accuracy and the speed are controlled.\n",
        "\n",
        ">Deep Residual Learning for Image Recognition Original link : <a href=\"https://arxiv.org/pdf/1512.03385.pdf\" target=\"_blank\">ResNet Paper</a>\n",
        "\n",
        "**The problem caused by increasing depth**\n",
        "\n",
        "* The first problem brought by increasing depth is the problem of gradient explosion / dissipation . This is because as the number of layers increases, the gradient of backpropagation in the network will become unstable with continuous multiplication, and become particularly large or special. small. Among them , the problem of gradient dissipation often occurs .\n",
        "\n",
        "* In order to overcome gradient dissipation, many solutions have been devised, such as using BatchNorm, replacing the activation function with ReLu, using Xaiver initialization, etc. It can be said that gradient dissipation has been well solved\n",
        "\n",
        "* Another problem of increasing depth is the problem of network degradation, that is, as the depth increases, the performance of the network will become worse and worse, which is directly reflected in the decrease in accuracy on the training set. The residual network article solves this problem. And after this problem is solved, the depth of the network has increased by several orders of magnitude.\n",
        "\n",
        "**Degradation of deep network**\n",
        "\n",
        ">With network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a favored deep model leads to higher training error.\n",
        "\n",
        "![residual](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/resnet/deg.jpg)\n",
        "\n",
        "* The above figure is the error rate of the training set classified by the network on the CIFAR10-data set with the increase of the network depth . It can be seen that if we directly stack the convolutional layers, as the number of layers increases, the error rate increases significantly. The trend is that the deepest 56-layer network has the worst accuracy . We verified it on the VGG network. For the CIFAR-10 dataset, it took 5 minutes on the 18-layer VGG network to get the full network training. The 80% accuracy rate was achieved, and the 34-layer VGG model took 8 minutes to get the 72% accuracy rate. The problem of network degradation does exist.\n",
        "\n",
        "* The decrease in the training set error rate indicates that the problem of degradation is not caused by overfitting. The specific reason is that it is left for further study. The author's other paper \"Identity Mappings in Deep Residual Networks\" proved the occurrence of degradation. It is because the optimization performance is not good, which indicates that the deeper the network, the more difficult the reverse gradient is to conduct.\n",
        "\n",
        "### Deep Residual Networks\n",
        "\n",
        "**From 10 to 100 layers**\n",
        "\n",
        "We can imagine that *when we simply stack the network directly to a particularly long length, the internal characteristics of the network have reached the best situation in one of the layers. At this time, the remaining layers should not make any changes to the characteristics and learn automatically. The form of identity mapping*. That is to say, for a particularly deep deep network, the solution space of the shallow form of the network should be a subset of the solution space of the deep network, in other words, a network deeper than the shallow network will not have at least Worse effect, but this is not true because of network degradation.\n",
        "\n",
        "Then, we settle for the second best. In the case of network degradation, if we do not add depth, we can improve the accuracy. Can we at least make the deep network achieve the same performance as the shallow network, that is, let the layers behind the deep network achieve at least The role of identity mapping . Based on this idea, the author proposes a residual module to help the network achieve identity mapping.\n",
        "\n",
        "`To understand ResNet, we must first understand what kind of problems will occur when the network becomes deeper.`\n",
        "\n",
        "**The first problem brought by increasing the network depth is the disappearance and explosion of the gradient.**\n",
        "\n",
        "This problem was successfully solved after Szegedy proposed the **BN (Batch Normalization)** structure. The BN layer can normalize the output of each layer. The size can still be kept stable after the reverse layer transfer, and it will not be too small or too large.\n",
        "\n",
        "**Is it easy to converge after adding BN and then increasing the depth?**\n",
        "\n",
        "The answer is still **negative**. The author mentioned the second problem-**the degradation problem**: when the level reaches a certain level, the accuracy will saturate and then decline rapidly. This decline is not caused by the disappearance of the gradient. It is not caused by overfit, but because the network is so complicated that it is difficult to achieve the ideal error rate by unconstrained stocking training alone.\n",
        "\n",
        "The degradation problem is not a problem of the network structure itself, but is caused by the current insufficient training methods. The currently widely used training methods, whether it is SGD, AdaGrad, or RMSProp, cannot reach the theoretically optimal convergence result after the network depth becomes larger.\n",
        "\n",
        "We can also prove that as long as there is an ideal training method, deeper networks will definitely perform better than shallow networks.\n",
        "\n",
        "*The proof process is also very simple*: `Suppose that several layers are added behind a network A to form a new network B. If the added level is just an identity mapping of the output of A, that is, the output of A is after the level of B becomes the output of B, there is no change, so the error rates of network A and network B are equal, which proves that the deepened network will not be worse than the network before deepening.`\n",
        "\n",
        "\n",
        "He Kaiming proposed a residual structure to implement the above identity mapping (Below Figure): In addition to the normal convolution layer output, the entire module has a branch directly connecting the input to the output. The output and the output of the convolution do The final output is obtained by arithmetic addition. The formula is H (x) = F (x) + x, x is the input, F (x) is the output of the convolution branch, and H (x) is the output of the entire structure. It can be shown that if all parameters in the F (x) branch are 0, H (x) is an identity mapping. The residual structure artificially creates an identity map, which can make the entire structure converge in the direction of the identity map, ensuring that the final error rate will not become worse because the depth becomes larger. If a network can achieve the desired result by simply setting the parameter values by hand, then this structure can easily converge to the result through training. This is a rule that is unsuccessful when designing complex networks. Recall that in order to restore the original distribution after BN processing, the formula y = rx + delta is used. When r is manually set to standard deviation and delta is the mean, y is the distribution before BN processing. This is the use of this Rules.\n",
        "\n",
        "\n",
        "![residual](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/resnet/residual.png)\n",
        "\n",
        "\n",
        "#### What does residual learning mean?\n",
        "\n",
        "The idea of residual learning is the above picture, which can be understood as a block, defined as follows:\n",
        "\n",
        "![residual](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/resnet/resnet11.gif)\n",
        "\n",
        "\n",
        "The residual learning block contains two branches or two mappings:\n",
        "\n",
        "1. Identity mapping refers to the curved curve on the right side of the figure above. As its name implies, identity mapping refers to its own mapping, which is x itself;\n",
        "\n",
        "\n",
        "2. *F*(x) Residual mapping refers to another branch, that is, part. This part is called residual mapping (y -x) .\n",
        "\n",
        "**What role does the residual module play in back propagation?**\n",
        "\n",
        "* The residual module will significantly reduce the parameter value in the module, so that the parameters in the network have a more sensitive response ability to the loss of reverse conduction, although the fundamental It does not solve the problem that the loss of backhaul is too small, but it reduces the parameters. Relatively speaking, it increases the effect of backhaul loss and also generates a certain regularization effect.\n",
        "\n",
        "* Secondly, because there are branches of the identity mapping in the forward process, the gradient conduction in the back-propagation process also has more simple paths , and the gradient can be transmitted to the previous module after only one relu.\n",
        "\n",
        "* The so-called backpropagation is that the network outputs a value, and then compares it with the real value to an error loss. At the same time, the loss is changed to change the parameter. The returned loss depends on the original loss and gradient. Since the purpose is to change the parameter, The problem is that if the intensity of changing the parameter is too small, the value of the parameter can be reduced, so that the loss of the intensity of changing the parameter is relatively greater.\n",
        "\n",
        "* Therefore, the most important role of the residual module is to change the way of forward and backward information transmission, thereby greatly promoting the optimization of the network.\n",
        "\n",
        "* Using the four criteria proposed by Inceptionv3, we will use them again to improve the residual module. Using criterion 3, the dimensionality reduction before spatial aggregation will not cause information loss, so the same method is also used here, adding 1 * 1 convolution The kernel is used to increase the non-linearity and reduce the depth of the output to reduce the computational cost. You get the form of a residual module that becomes a bottleneck. The figure above shows the basic form on the left and the bottleneck form on the right.\n",
        "\n",
        "* To sum up, the shortcut module will help the features in the network perform identity mapping in the forward process, and help conduct gradients in the reverse process, so that deeper models can be successfully trained.\n",
        "\n",
        "\n",
        "#### Why can the residual learning solve the problem of \"the accuracy of the network deepening declines\"?\n",
        "\n",
        "For a neural network model, if the model is optimal, then training can easily optimize the residual mapping to 0, and only identity mapping is left at this time. No matter how you increase the depth, the network will always be in an optimal state in theory. Because it is equivalent to all the subsequent added networks to carry information transmission along the identity mapping (self), it can be understood that the number of layers behind the optimal network is discarded (without the ability to extract features), and it does not actually play a role. . In this way, the performance of the network will not decrease with increasing depth.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The author used two types of data, **ImageNet** and **CIFAR**, to prove the effectiveness of ResNet:\n",
        "\n",
        "The first is ImageNet. The authors compared the training effect of ResNet structure and traditional structure with the same number of layers. The left side of Figure is a VGG-19 network with a traditional structure (each followed by BN), the middle is a 34-layer network with a traditional structure (each followed by BN), and the right side is 34 layers ResNet (the solid line indicates a direct connection, and the dashed line indicates a dimensional change using 1x1 convolution to match the number of features of the input and output). Figure 3 shows the results after training these types of networks.\n",
        "\n",
        "\n",
        "The data on the left shows that the 34-layer network (red line) with the traditional structure has a higher error rate than the VGG-19 (blue-green line). Because the BN structure is added to each layer Therefore, the high error is not caused by the gradient disappearing after the level is increased, but by the degradation problem; the ResNet structure on the right side of Figure 3 shows that the 34-layer network (red line) has a higher error rate than the 18-layer network (blue-green line). Low, this is because the ResNet structure has overcome the degradation problem. In addition, the final error rate of the ResNet 18-layer network on the right is similar to the error rate of the traditional 18-layer network on the left. This is because the 18-layer network is simpler and can converge to a more ideal result even without the ResNet structure.\n",
        "\n",
        "\n",
        "![residual](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/resnet/resnetnetwork.png)\n",
        "\n",
        "\n",
        "![residual](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/resnet/error.png)\n",
        "\n",
        "\n",
        "The ResNet structure like the left side of Fig. 4 is only used for shallow ResNet networks. If there are many network layers, the dimensions near the output end of the network will be very large. Still using the structure on the left side of Fig. 4 will cause a huge amount of calculation. For deeper networks, we all use the bottleneck structure on the right side of Figure 4, first using a 1x1 convolution for dimensionality reduction, then 3x3 convolution, and finally using 1x1 dimensionality to restore the original dimension.\n",
        "\n",
        "In practice, considering the cost of the calculation, the residual block is calculated and optimized, that is, the two 3x3 convolution layers are replaced with 1x1 + 3x3 + 1x1 , as shown below. The middle 3x3 convolutional layer in the new structure first reduces the calculation under one dimensionality-reduced 1x1 convolutional layer , and then restores it under another 1x1 convolutional layer , both maintaining accuracy and reducing the amount of calculation .\n",
        "\n",
        "![residual](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/resnet/variant.png)\n",
        "\n",
        "\n",
        "This is equivalent to reducing the amount of parameters for the same number of layers , so it can be extended to deeper models. So the author proposed ResNet with 50, 101 , and 152 layers , and not only did not have degradation problems, the error rate was greatly reduced, and the computational complexity was also kept at a very low level .\n",
        "\n",
        "At this time, the error rate of ResNet has already dropped other networks a few streets, but it does not seem to be satisfied. Therefore, a more abnormal 1202 layer network has been built. For such a deep network, optimization is still not difficult, but it appears The problem of overfitting is quite normal. The author also said that the 1202 layer model will be further improved in the future.\n",
        "\n",
        "**Diffrent Variants** : -\n",
        "\n",
        "![residual](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/resnet/resnetnetwork.png)\n",
        "\n",
        "\n",
        "Below is the transcript of resnet, winning the championship at ImageNet2015\n",
        "\n",
        "![residual](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/resnet/resnetresults.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp3Acrr36H0-"
      },
      "source": [
        "# Code Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1cJ2a3y6RMV"
      },
      "source": [
        "## From Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cUP9JAzOW2qn"
      },
      "outputs": [],
      "source": [
        "# Importing the required library\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, AveragePooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBXRoewzW_yK",
        "outputId": "110980be-f049-4ae1-9bff-cbb70e8fbbc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 50000\n",
            "Number of testing samples: 10000\n"
          ]
        }
      ],
      "source": [
        "# Getting Data\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# Loading the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Printing some information about the dataset\n",
        "print(f\"Number of training samples: {len(x_train)}\")\n",
        "print(f\"Number of testing samples: {len(x_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1Sp5d1RXHyS",
        "outputId": "0eb9dc86-178f-4ba2-b2b6-7743e9ad3a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert labels to one-hot encoding\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Bsl9_lUu3_pC"
      },
      "outputs": [],
      "source": [
        "# Residual block\n",
        "def residual_block(x, filters, downsample=False):\n",
        "    strides = (2, 2) if downsample else (1, 1)\n",
        "\n",
        "    # First convolutional layer of the block\n",
        "    y = Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(x)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "    # Second convolutional layer of the block\n",
        "    y = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "\n",
        "    # Skip connection if downsample or number of filters change\n",
        "    if downsample:\n",
        "        x = Conv2D(filters, kernel_size=(1, 1), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    # Add skip connection\n",
        "    y = Add()([x, y])\n",
        "    y = Activation('relu')(y)\n",
        "    return y\n",
        "\n",
        "# Build the ResNet model\n",
        "def resnet(input_shape, num_classes):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Initial convolutional layer\n",
        "    x = Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Residual blocks\n",
        "    x = residual_block(x, filters=16)\n",
        "    x = residual_block(x, filters=16)\n",
        "    x = residual_block(x, filters=32, downsample=True)\n",
        "    x = residual_block(x, filters=32)\n",
        "    x = residual_block(x, filters=64, downsample=True)\n",
        "    x = residual_block(x, filters=64)\n",
        "\n",
        "    # Average pooling and output layer\n",
        "    x = AveragePooling2D(pool_size=(8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjInuYeJXfpN",
        "outputId": "ebefd213-4177-45e9-d3f9-726639a87e89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:From c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 60s 78ms/step - loss: 1.3750 - accuracy: 0.4992 - val_loss: 1.5199 - val_accuracy: 0.4885\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.9613 - accuracy: 0.6575"
          ]
        }
      ],
      "source": [
        "# Create the ResNet model\n",
        "model = resnet(input_shape=(32, 32, 3), num_classes=10)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=5, verbose=1, validation_split=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6UkVWtB66xM"
      },
      "source": [
        "## Pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "vgGUYrE9X1jJ",
        "outputId": "96dcc88f-4539-4083-823e-2433aa2e848f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12jiQxJzYSYl3wnC8x5wHAhRzzJmmsCXP\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?/export=download&id=12jiQxJzYSYl3wnC8x5wHAhRzzJmmsCXP\n",
            "To: /content/catdog.zip\n",
            "100%|██████████| 9.09M/9.09M [00:00<00:00, 113MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'catdog.zip'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download the data from g drive\n",
        "\n",
        "import gdown\n",
        "url = \"https://drive.google.com/file/d/12jiQxJzYSYl3wnC8x5wHAhRzzJmmsCXP/view?usp=sharing\"\n",
        "file_id = url.split(\"/\")[-2]\n",
        "print(file_id)\n",
        "prefix = 'https://drive.google.com/uc?/export=download&id='\n",
        "gdown.download(prefix+file_id, \"catdog.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx_SPTBRX4wb",
        "outputId": "8b08597e-9c1b-4bd6-e83a-7fd5996a3a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  catdog.zip\n",
            "   creating: train/\n",
            "   creating: train/Cat/\n",
            "  inflating: train/Cat/0.jpg         \n",
            "  inflating: train/Cat/1.jpg         \n",
            "  inflating: train/Cat/2.jpg         \n",
            "  inflating: train/Cat/cat.2405.jpg  \n",
            "  inflating: train/Cat/cat.2406.jpg  \n",
            "  inflating: train/Cat/cat.2436.jpg  \n",
            "  inflating: train/Cat/cat.2437.jpg  \n",
            "  inflating: train/Cat/cat.2438.jpg  \n",
            "  inflating: train/Cat/cat.2439.jpg  \n",
            "  inflating: train/Cat/cat.2440.jpg  \n",
            "  inflating: train/Cat/cat.2441.jpg  \n",
            "  inflating: train/Cat/cat.2442.jpg  \n",
            "  inflating: train/Cat/cat.2443.jpg  \n",
            "  inflating: train/Cat/cat.2444.jpg  \n",
            "  inflating: train/Cat/cat.2445.jpg  \n",
            "  inflating: train/Cat/cat.2446.jpg  \n",
            "  inflating: train/Cat/cat.2447.jpg  \n",
            "  inflating: train/Cat/cat.2448.jpg  \n",
            "  inflating: train/Cat/cat.2449.jpg  \n",
            "  inflating: train/Cat/cat.2450.jpg  \n",
            "  inflating: train/Cat/cat.2451.jpg  \n",
            "  inflating: train/Cat/cat.2452.jpg  \n",
            "  inflating: train/Cat/cat.2453.jpg  \n",
            "  inflating: train/Cat/cat.2454.jpg  \n",
            "  inflating: train/Cat/cat.2455.jpg  \n",
            "  inflating: train/Cat/cat.2456.jpg  \n",
            "  inflating: train/Cat/cat.2457.jpg  \n",
            "  inflating: train/Cat/cat.2458.jpg  \n",
            "  inflating: train/Cat/cat.2459.jpg  \n",
            "  inflating: train/Cat/cat.2460.jpg  \n",
            "  inflating: train/Cat/cat.2461.jpg  \n",
            "  inflating: train/Cat/cat.2462.jpg  \n",
            "  inflating: train/Cat/cat.2463.jpg  \n",
            "  inflating: train/Cat/cat.2464.jpg  \n",
            "  inflating: train/Cat/cat.855.jpg   \n",
            "  inflating: train/Cat/cat.856.jpg   \n",
            "  inflating: train/Cat/cat.857.jpg   \n",
            "  inflating: train/Cat/cat.858.jpg   \n",
            "  inflating: train/Cat/cat.859.jpg   \n",
            "  inflating: train/Cat/cat.86.jpg    \n",
            "  inflating: train/Cat/cat.860.jpg   \n",
            "  inflating: train/Cat/cat.861.jpg   \n",
            "  inflating: train/Cat/cat.862.jpg   \n",
            "  inflating: train/Cat/cat.863.jpg   \n",
            "  inflating: train/Cat/cat.864.jpg   \n",
            "  inflating: train/Cat/cat.865.jpg   \n",
            "  inflating: train/Cat/cat.866.jpg   \n",
            "  inflating: train/Cat/cat.867.jpg   \n",
            "  inflating: train/Cat/cat.868.jpg   \n",
            "  inflating: train/Cat/cat.869.jpg   \n",
            "  inflating: train/Cat/cat.87.jpg    \n",
            "  inflating: train/Cat/cat.870.jpg   \n",
            "  inflating: train/Cat/cat.871.jpg   \n",
            "  inflating: train/Cat/cat.872.jpg   \n",
            "  inflating: train/Cat/cat.873.jpg   \n",
            "  inflating: train/Cat/cat.874.jpg   \n",
            "  inflating: train/Cat/cat.875.jpg   \n",
            "  inflating: train/Cat/cat.876.jpg   \n",
            "  inflating: train/Cat/cat.877.jpg   \n",
            "  inflating: train/Cat/cat.878.jpg   \n",
            "  inflating: train/Cat/cat.879.jpg   \n",
            "  inflating: train/Cat/cat.88.jpg    \n",
            "  inflating: train/Cat/cat.880.jpg   \n",
            "  inflating: train/Cat/cat.881.jpg   \n",
            "  inflating: train/Cat/cat.882.jpg   \n",
            "  inflating: train/Cat/cat.883.jpg   \n",
            "  inflating: train/Cat/cat.884.jpg   \n",
            "  inflating: train/Cat/cat.885.jpg   \n",
            "  inflating: train/Cat/cat.886.jpg   \n",
            "  inflating: train/Cat/cat.887.jpg   \n",
            "  inflating: train/Cat/cat.888.jpg   \n",
            "  inflating: train/Cat/cat.889.jpg   \n",
            "  inflating: train/Cat/cat.89.jpg    \n",
            "  inflating: train/Cat/cat.890.jpg   \n",
            "  inflating: train/Cat/cat.891.jpg   \n",
            "  inflating: train/Cat/cat.892.jpg   \n",
            "  inflating: train/Cat/cat.893.jpg   \n",
            "  inflating: train/Cat/cat.894.jpg   \n",
            "  inflating: train/Cat/cat.895.jpg   \n",
            "  inflating: train/Cat/cat.896.jpg   \n",
            "  inflating: train/Cat/cat.897.jpg   \n",
            "  inflating: train/Cat/cat.898.jpg   \n",
            "  inflating: train/Cat/cat.899.jpg   \n",
            "  inflating: train/Cat/cat.9.jpg     \n",
            "  inflating: train/Cat/cat.90.jpg    \n",
            "  inflating: train/Cat/cat.900.jpg   \n",
            "  inflating: train/Cat/cat.901.jpg   \n",
            "  inflating: train/Cat/cat.902.jpg   \n",
            "  inflating: train/Cat/cat.903.jpg   \n",
            "  inflating: train/Cat/cat.904.jpg   \n",
            "  inflating: train/Cat/cat.905.jpg   \n",
            "  inflating: train/Cat/cat.906.jpg   \n",
            "  inflating: train/Cat/cat.907.jpg   \n",
            "  inflating: train/Cat/cat.908.jpg   \n",
            "  inflating: train/Cat/cat.909.jpg   \n",
            "  inflating: train/Cat/cat.91.jpg    \n",
            "  inflating: train/Cat/cat.910.jpg   \n",
            "  inflating: train/Cat/cat.911.jpg   \n",
            "  inflating: train/Cat/cat.912.jpg   \n",
            "  inflating: train/Cat/cat.913.jpg   \n",
            "  inflating: train/Cat/cat.914.jpg   \n",
            "  inflating: train/Cat/cat.915.jpg   \n",
            "  inflating: train/Cat/cat.916.jpg   \n",
            "  inflating: train/Cat/cat.917.jpg   \n",
            "  inflating: train/Cat/cat.918.jpg   \n",
            "  inflating: train/Cat/cat.919.jpg   \n",
            "  inflating: train/Cat/cat.92.jpg    \n",
            "  inflating: train/Cat/cat.920.jpg   \n",
            "  inflating: train/Cat/cat.93.jpg    \n",
            "  inflating: train/Cat/cat.94.jpg    \n",
            "  inflating: train/Cat/cat.946.jpg   \n",
            "  inflating: train/Cat/cat.947.jpg   \n",
            "  inflating: train/Cat/cat.948.jpg   \n",
            "  inflating: train/Cat/cat.949.jpg   \n",
            "  inflating: train/Cat/cat.95.jpg    \n",
            "  inflating: train/Cat/cat.950.jpg   \n",
            "  inflating: train/Cat/cat.951.jpg   \n",
            "  inflating: train/Cat/cat.952.jpg   \n",
            "  inflating: train/Cat/cat.953.jpg   \n",
            "  inflating: train/Cat/cat.954.jpg   \n",
            "  inflating: train/Cat/cat.955.jpg   \n",
            "  inflating: train/Cat/cat.956.jpg   \n",
            "  inflating: train/Cat/cat.957.jpg   \n",
            "  inflating: train/Cat/cat.958.jpg   \n",
            "  inflating: train/Cat/cat.959.jpg   \n",
            "  inflating: train/Cat/cat.96.jpg    \n",
            "  inflating: train/Cat/cat.960.jpg   \n",
            "  inflating: train/Cat/cat.961.jpg   \n",
            "  inflating: train/Cat/cat.962.jpg   \n",
            "  inflating: train/Cat/cat.963.jpg   \n",
            "  inflating: train/Cat/cat.964.jpg   \n",
            "  inflating: train/Cat/cat.965.jpg   \n",
            "  inflating: train/Cat/cat.966.jpg   \n",
            "  inflating: train/Cat/cat.967.jpg   \n",
            "  inflating: train/Cat/cat.968.jpg   \n",
            "  inflating: train/Cat/cat.969.jpg   \n",
            "  inflating: train/Cat/cat.97.jpg    \n",
            "  inflating: train/Cat/cat.970.jpg   \n",
            "  inflating: train/Cat/cat.971.jpg   \n",
            "  inflating: train/Cat/cat.972.jpg   \n",
            "  inflating: train/Cat/cat.973.jpg   \n",
            "  inflating: train/Cat/cat.974.jpg   \n",
            "  inflating: train/Cat/cat.975.jpg   \n",
            "  inflating: train/Cat/cat.976.jpg   \n",
            "  inflating: train/Cat/cat.977.jpg   \n",
            "  inflating: train/Cat/cat.978.jpg   \n",
            "  inflating: train/Cat/cat.979.jpg   \n",
            "  inflating: train/Cat/cat.98.jpg    \n",
            "  inflating: train/Cat/cat.980.jpg   \n",
            "  inflating: train/Cat/cat.981.jpg   \n",
            "  inflating: train/Cat/cat.982.jpg   \n",
            "  inflating: train/Cat/cat.983.jpg   \n",
            "  inflating: train/Cat/cat.984.jpg   \n",
            "  inflating: train/Cat/cat.985.jpg   \n",
            "  inflating: train/Cat/cat.986.jpg   \n",
            "  inflating: train/Cat/cat.987.jpg   \n",
            "  inflating: train/Cat/cat.988.jpg   \n",
            "  inflating: train/Cat/cat.989.jpg   \n",
            "  inflating: train/Cat/cat.99.jpg    \n",
            "  inflating: train/Cat/cat.990.jpg   \n",
            "  inflating: train/Cat/cat.991.jpg   \n",
            "  inflating: train/Cat/cat.992.jpg   \n",
            "  inflating: train/Cat/cat.993.jpg   \n",
            "  inflating: train/Cat/cat.994.jpg   \n",
            "  inflating: train/Cat/cat.995.jpg   \n",
            "  inflating: train/Cat/cat.996.jpg   \n",
            "  inflating: train/Cat/cat.997.jpg   \n",
            "  inflating: train/Cat/cat.998.jpg   \n",
            "  inflating: train/Cat/cat.999.jpg   \n",
            "   creating: train/Dog/\n",
            "  inflating: train/Dog/10493.jpg     \n",
            "  inflating: train/Dog/11785.jpg     \n",
            "  inflating: train/Dog/9839.jpg      \n",
            "  inflating: train/Dog/dog.2432.jpg  \n",
            "  inflating: train/Dog/dog.2433.jpg  \n",
            "  inflating: train/Dog/dog.2434.jpg  \n",
            "  inflating: train/Dog/dog.2435.jpg  \n",
            "  inflating: train/Dog/dog.2436.jpg  \n",
            "  inflating: train/Dog/dog.2437.jpg  \n",
            "  inflating: train/Dog/dog.2438.jpg  \n",
            "  inflating: train/Dog/dog.2439.jpg  \n",
            "  inflating: train/Dog/dog.2440.jpg  \n",
            "  inflating: train/Dog/dog.2441.jpg  \n",
            "  inflating: train/Dog/dog.2442.jpg  \n",
            "  inflating: train/Dog/dog.2443.jpg  \n",
            "  inflating: train/Dog/dog.2444.jpg  \n",
            "  inflating: train/Dog/dog.2445.jpg  \n",
            "  inflating: train/Dog/dog.2446.jpg  \n",
            "  inflating: train/Dog/dog.2447.jpg  \n",
            "  inflating: train/Dog/dog.2448.jpg  \n",
            "  inflating: train/Dog/dog.2449.jpg  \n",
            "  inflating: train/Dog/dog.2450.jpg  \n",
            "  inflating: train/Dog/dog.2451.jpg  \n",
            "  inflating: train/Dog/dog.2452.jpg  \n",
            "  inflating: train/Dog/dog.2453.jpg  \n",
            "  inflating: train/Dog/dog.2454.jpg  \n",
            "  inflating: train/Dog/dog.2455.jpg  \n",
            "  inflating: train/Dog/dog.2456.jpg  \n",
            "  inflating: train/Dog/dog.2457.jpg  \n",
            "  inflating: train/Dog/dog.2458.jpg  \n",
            "  inflating: train/Dog/dog.2459.jpg  \n",
            "  inflating: train/Dog/dog.2460.jpg  \n",
            "  inflating: train/Dog/dog.2461.jpg  \n",
            "  inflating: train/Dog/dog.844.jpg   \n",
            "  inflating: train/Dog/dog.845.jpg   \n",
            "  inflating: train/Dog/dog.846.jpg   \n",
            "  inflating: train/Dog/dog.847.jpg   \n",
            "  inflating: train/Dog/dog.848.jpg   \n",
            "  inflating: train/Dog/dog.849.jpg   \n",
            "  inflating: train/Dog/dog.85.jpg    \n",
            "  inflating: train/Dog/dog.850.jpg   \n",
            "  inflating: train/Dog/dog.851.jpg   \n",
            "  inflating: train/Dog/dog.852.jpg   \n",
            "  inflating: train/Dog/dog.853.jpg   \n",
            "  inflating: train/Dog/dog.854.jpg   \n",
            "  inflating: train/Dog/dog.855.jpg   \n",
            "  inflating: train/Dog/dog.856.jpg   \n",
            "  inflating: train/Dog/dog.857.jpg   \n",
            "  inflating: train/Dog/dog.858.jpg   \n",
            "  inflating: train/Dog/dog.859.jpg   \n",
            "  inflating: train/Dog/dog.86.jpg    \n",
            "  inflating: train/Dog/dog.860.jpg   \n",
            "  inflating: train/Dog/dog.861.jpg   \n",
            "  inflating: train/Dog/dog.862.jpg   \n",
            "  inflating: train/Dog/dog.863.jpg   \n",
            "  inflating: train/Dog/dog.864.jpg   \n",
            "  inflating: train/Dog/dog.865.jpg   \n",
            "  inflating: train/Dog/dog.866.jpg   \n",
            "  inflating: train/Dog/dog.867.jpg   \n",
            "  inflating: train/Dog/dog.868.jpg   \n",
            "  inflating: train/Dog/dog.869.jpg   \n",
            "  inflating: train/Dog/dog.87.jpg    \n",
            "  inflating: train/Dog/dog.870.jpg   \n",
            "  inflating: train/Dog/dog.871.jpg   \n",
            "  inflating: train/Dog/dog.872.jpg   \n",
            "  inflating: train/Dog/dog.873.jpg   \n",
            "  inflating: train/Dog/dog.874.jpg   \n",
            "  inflating: train/Dog/dog.875.jpg   \n",
            "  inflating: train/Dog/dog.876.jpg   \n",
            "  inflating: train/Dog/dog.877.jpg   \n",
            "  inflating: train/Dog/dog.878.jpg   \n",
            "  inflating: train/Dog/dog.879.jpg   \n",
            "  inflating: train/Dog/dog.88.jpg    \n",
            "  inflating: train/Dog/dog.880.jpg   \n",
            "  inflating: train/Dog/dog.881.jpg   \n",
            "  inflating: train/Dog/dog.882.jpg   \n",
            "  inflating: train/Dog/dog.883.jpg   \n",
            "  inflating: train/Dog/dog.884.jpg   \n",
            "  inflating: train/Dog/dog.885.jpg   \n",
            "  inflating: train/Dog/dog.886.jpg   \n",
            "  inflating: train/Dog/dog.887.jpg   \n",
            "  inflating: train/Dog/dog.888.jpg   \n",
            "  inflating: train/Dog/dog.889.jpg   \n",
            "  inflating: train/Dog/dog.89.jpg    \n",
            "  inflating: train/Dog/dog.890.jpg   \n",
            "  inflating: train/Dog/dog.891.jpg   \n",
            "  inflating: train/Dog/dog.892.jpg   \n",
            "  inflating: train/Dog/dog.893.jpg   \n",
            "  inflating: train/Dog/dog.894.jpg   \n",
            "  inflating: train/Dog/dog.895.jpg   \n",
            "  inflating: train/Dog/dog.896.jpg   \n",
            "  inflating: train/Dog/dog.897.jpg   \n",
            "  inflating: train/Dog/dog.898.jpg   \n",
            "  inflating: train/Dog/dog.9.jpg     \n",
            "  inflating: train/Dog/dog.90.jpg    \n",
            "  inflating: train/Dog/dog.91.jpg    \n",
            "  inflating: train/Dog/dog.92.jpg    \n",
            "  inflating: train/Dog/dog.93.jpg    \n",
            "  inflating: train/Dog/dog.936.jpg   \n",
            "  inflating: train/Dog/dog.937.jpg   \n",
            "  inflating: train/Dog/dog.938.jpg   \n",
            "  inflating: train/Dog/dog.939.jpg   \n",
            "  inflating: train/Dog/dog.94.jpg    \n",
            "  inflating: train/Dog/dog.940.jpg   \n",
            "  inflating: train/Dog/dog.941.jpg   \n",
            "  inflating: train/Dog/dog.942.jpg   \n",
            "  inflating: train/Dog/dog.943.jpg   \n",
            "  inflating: train/Dog/dog.944.jpg   \n",
            "  inflating: train/Dog/dog.945.jpg   \n",
            "  inflating: train/Dog/dog.946.jpg   \n",
            "  inflating: train/Dog/dog.947.jpg   \n",
            "  inflating: train/Dog/dog.948.jpg   \n",
            "  inflating: train/Dog/dog.949.jpg   \n",
            "  inflating: train/Dog/dog.95.jpg    \n",
            "  inflating: train/Dog/dog.950.jpg   \n",
            "  inflating: train/Dog/dog.951.jpg   \n",
            "  inflating: train/Dog/dog.952.jpg   \n",
            "  inflating: train/Dog/dog.953.jpg   \n",
            "  inflating: train/Dog/dog.954.jpg   \n",
            "  inflating: train/Dog/dog.955.jpg   \n",
            "  inflating: train/Dog/dog.956.jpg   \n",
            "  inflating: train/Dog/dog.957.jpg   \n",
            "  inflating: train/Dog/dog.958.jpg   \n",
            "  inflating: train/Dog/dog.959.jpg   \n",
            "  inflating: train/Dog/dog.96.jpg    \n",
            "  inflating: train/Dog/dog.960.jpg   \n",
            "  inflating: train/Dog/dog.961.jpg   \n",
            "  inflating: train/Dog/dog.962.jpg   \n",
            "  inflating: train/Dog/dog.963.jpg   \n",
            "  inflating: train/Dog/dog.964.jpg   \n",
            "  inflating: train/Dog/dog.965.jpg   \n",
            "  inflating: train/Dog/dog.966.jpg   \n",
            "  inflating: train/Dog/dog.967.jpg   \n",
            "  inflating: train/Dog/dog.968.jpg   \n",
            "  inflating: train/Dog/dog.969.jpg   \n",
            "  inflating: train/Dog/dog.97.jpg    \n",
            "  inflating: train/Dog/dog.970.jpg   \n",
            "  inflating: train/Dog/dog.971.jpg   \n",
            "  inflating: train/Dog/dog.972.jpg   \n",
            "  inflating: train/Dog/dog.973.jpg   \n",
            "  inflating: train/Dog/dog.974.jpg   \n",
            "  inflating: train/Dog/dog.975.jpg   \n",
            "  inflating: train/Dog/dog.976.jpg   \n",
            "  inflating: train/Dog/dog.977.jpg   \n",
            "  inflating: train/Dog/dog.978.jpg   \n",
            "  inflating: train/Dog/dog.979.jpg   \n",
            "  inflating: train/Dog/dog.98.jpg    \n",
            "  inflating: train/Dog/dog.980.jpg   \n",
            "  inflating: train/Dog/dog.981.jpg   \n",
            "  inflating: train/Dog/dog.982.jpg   \n",
            "  inflating: train/Dog/dog.983.jpg   \n",
            "  inflating: train/Dog/dog.984.jpg   \n",
            "  inflating: train/Dog/dog.985.jpg   \n",
            "  inflating: train/Dog/dog.986.jpg   \n",
            "  inflating: train/Dog/dog.987.jpg   \n",
            "  inflating: train/Dog/dog.988.jpg   \n",
            "  inflating: train/Dog/dog.989.jpg   \n",
            "  inflating: train/Dog/dog.99.jpg    \n",
            "  inflating: train/Dog/dog.990.jpg   \n",
            "  inflating: train/Dog/dog.991.jpg   \n",
            "  inflating: train/Dog/dog.992.jpg   \n",
            "  inflating: train/Dog/dog.993.jpg   \n",
            "  inflating: train/Dog/dog.994.jpg   \n",
            "  inflating: train/Dog/dog.995.jpg   \n",
            "  inflating: train/Dog/dog.996.jpg   \n",
            "  inflating: train/Dog/dog.997.jpg   \n",
            "  inflating: train/Dog/dog.998.jpg   \n",
            "  inflating: train/Dog/dog.999.jpg   \n",
            "   creating: validation/\n",
            "   creating: validation/Cat/\n",
            "  inflating: validation/Cat/cat.2407.jpg  \n",
            "  inflating: validation/Cat/cat.2408.jpg  \n",
            "  inflating: validation/Cat/cat.2409.jpg  \n",
            "  inflating: validation/Cat/cat.2410.jpg  \n",
            "  inflating: validation/Cat/cat.2411.jpg  \n",
            "  inflating: validation/Cat/cat.2412.jpg  \n",
            "  inflating: validation/Cat/cat.2413.jpg  \n",
            "  inflating: validation/Cat/cat.2414.jpg  \n",
            "  inflating: validation/Cat/cat.2415.jpg  \n",
            "  inflating: validation/Cat/cat.2416.jpg  \n",
            "  inflating: validation/Cat/cat.2417.jpg  \n",
            "  inflating: validation/Cat/cat.2418.jpg  \n",
            "  inflating: validation/Cat/cat.2419.jpg  \n",
            "  inflating: validation/Cat/cat.2420.jpg  \n",
            "  inflating: validation/Cat/cat.2421.jpg  \n",
            "  inflating: validation/Cat/cat.2422.jpg  \n",
            "  inflating: validation/Cat/cat.2423.jpg  \n",
            "  inflating: validation/Cat/cat.2424.jpg  \n",
            "  inflating: validation/Cat/cat.2425.jpg  \n",
            "  inflating: validation/Cat/cat.2426.jpg  \n",
            "  inflating: validation/Cat/cat.2427.jpg  \n",
            "  inflating: validation/Cat/cat.2428.jpg  \n",
            "  inflating: validation/Cat/cat.2429.jpg  \n",
            "  inflating: validation/Cat/cat.2430.jpg  \n",
            "  inflating: validation/Cat/cat.2431.jpg  \n",
            "  inflating: validation/Cat/cat.2432.jpg  \n",
            "  inflating: validation/Cat/cat.2433.jpg  \n",
            "  inflating: validation/Cat/cat.2434.jpg  \n",
            "  inflating: validation/Cat/cat.2435.jpg  \n",
            "   creating: validation/Dog/\n",
            "  inflating: validation/Dog/dog.2402.jpg  \n",
            "  inflating: validation/Dog/dog.2403.jpg  \n",
            "  inflating: validation/Dog/dog.2404.jpg  \n",
            "  inflating: validation/Dog/dog.2405.jpg  \n",
            "  inflating: validation/Dog/dog.2406.jpg  \n",
            "  inflating: validation/Dog/dog.2407.jpg  \n",
            "  inflating: validation/Dog/dog.2408.jpg  \n",
            "  inflating: validation/Dog/dog.2409.jpg  \n",
            "  inflating: validation/Dog/dog.2410.jpg  \n",
            "  inflating: validation/Dog/dog.2411.jpg  \n",
            "  inflating: validation/Dog/dog.2412.jpg  \n",
            "  inflating: validation/Dog/dog.2413.jpg  \n",
            "  inflating: validation/Dog/dog.2414.jpg  \n",
            "  inflating: validation/Dog/dog.2415.jpg  \n",
            "  inflating: validation/Dog/dog.2416.jpg  \n",
            "  inflating: validation/Dog/dog.2417.jpg  \n",
            "  inflating: validation/Dog/dog.2418.jpg  \n",
            "  inflating: validation/Dog/dog.2419.jpg  \n",
            "  inflating: validation/Dog/dog.2420.jpg  \n",
            "  inflating: validation/Dog/dog.2421.jpg  \n",
            "  inflating: validation/Dog/dog.2422.jpg  \n",
            "  inflating: validation/Dog/dog.2423.jpg  \n",
            "  inflating: validation/Dog/dog.2424.jpg  \n",
            "  inflating: validation/Dog/dog.2425.jpg  \n",
            "  inflating: validation/Dog/dog.2426.jpg  \n",
            "  inflating: validation/Dog/dog.2427.jpg  \n",
            "  inflating: validation/Dog/dog.2428.jpg  \n",
            "  inflating: validation/Dog/dog.2429.jpg  \n",
            "  inflating: validation/Dog/dog.2430.jpg  \n",
            "  inflating: validation/Dog/dog.2431.jpg  \n"
          ]
        }
      ],
      "source": [
        "!unzip catdog.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MR9OoLQmX8VJ",
        "outputId": "e9695829-98f8-4eba-ff44-5f5226a0577a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 6s 0us/step\n",
            "Found 337 images belonging to 2 classes.\n",
            "Found 59 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "125/125 [==============================] - 19s 134ms/step - batch: 62.0000 - size: 15.2800 - loss: 1.5985 - acc: 0.9497 - val_loss: 2.8061 - val_acc: 0.9311\n",
            "Epoch 2/5\n",
            "125/125 [==============================] - 15s 118ms/step - batch: 62.0000 - size: 15.4000 - loss: 0.1633 - acc: 0.9906 - val_loss: 2.4300 - val_acc: 0.9486\n",
            "Epoch 3/5\n",
            "125/125 [==============================] - 16s 129ms/step - batch: 62.0000 - size: 15.2800 - loss: 0.0889 - acc: 0.9916 - val_loss: 2.3081 - val_acc: 0.9649\n",
            "Epoch 4/5\n",
            "125/125 [==============================] - 15s 119ms/step - batch: 62.0000 - size: 15.2800 - loss: 0.0966 - acc: 0.9911 - val_loss: 1.9620 - val_acc: 0.9176\n",
            "Epoch 5/5\n",
            "125/125 [==============================] - 15s 118ms/step - batch: 62.0000 - size: 15.2800 - loss: 0.0648 - acc: 0.9953 - val_loss: 2.2139 - val_acc: 0.9676\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "\n",
        "# Set the path to your training and validation data\n",
        "train_data_dir = '/content/train'\n",
        "validation_data_dir = '/content/validation'\n",
        "\n",
        "# Set the number of training and validation samples\n",
        "num_train_samples = 2000\n",
        "num_validation_samples = 800\n",
        "\n",
        "# Set the number of epochs and batch size\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "\n",
        "# Load the VGG16 model without the top layer\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the base model as a layer\n",
        "model.add(base_model)\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Preprocess the training and validation data\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=num_train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=num_validation_samples // batch_size)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('dog_cat_classifier.h5')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
